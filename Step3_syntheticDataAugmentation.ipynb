{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook generates synthetic sentences to agument the LSTM's training data (for BOTH of the train/test partitions and\n",
    "#ALL ten sessions). Step 3 utilizes the data labels created during Step 2 to rearrange the data into new sentences. \n",
    "#The output of Step 3 is a set of .tfrecord files that are mixed together with the real data during LSTM training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#suppress all tensorflow warnings (largely related to compatability with v2)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition \n",
    "from characterDefinitions import getHandwritingCharacterDefinitions\n",
    "from makeSyntheticSentences import generateCharacterSequences, extractCharacterSnippets, addSingleLetterSnippets\n",
    "import multiprocessing\n",
    "import datetime\n",
    "from dataPreprocessing import normalizeSentenceDataCube\n",
    "\n",
    "#point this towards the top level dataset directory\n",
    "rootDir = os.path.expanduser('~') + '/handwritingBCIData/'\n",
    "\n",
    "#define which datasets to process\n",
    "dataDirs = ['t5.2019.05.08','t5.2019.11.25','t5.2019.12.09','t5.2019.12.11','t5.2019.12.18',\n",
    "            't5.2019.12.20','t5.2020.01.06','t5.2020.01.08','t5.2020.01.13','t5.2020.01.15']\n",
    "\n",
    "#construct synthetic data for both training partitions\n",
    "cvParts = ['HeldOutBlocks', 'HeldOutTrials']\n",
    "\n",
    "#defines the list of all 31 characters and what to call them\n",
    "charDef = getHandwritingCharacterDefinitions()\n",
    "\n",
    "#saves all synthetic sentences & snippet libraries in this folder\n",
    "if not os.path.isdir(rootDir + 'LSTMTrainingSteps/Step3_SyntheticSentences'):\n",
    "    os.mkdir(rootDir + 'LSTMTrainingSteps/Step3_SyntheticSentences')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing t5.2019.05.08\n",
      "--HeldOutBlocks\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m labels \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mloadmat(rootDir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLSTMTrainingSteps/Step2_HMMLabels/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mcvPart\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mdataDir\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_timeSeriesLabels.mat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[39m#cut out character snippets from the data for augmentation\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m snippetDict \u001b[39m=\u001b[39m extractCharacterSnippets(labels[\u001b[39m'\u001b[39;49m\u001b[39mletterStarts\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     30\u001b[0m                                        labels[\u001b[39m'\u001b[39;49m\u001b[39mblankWindows\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m     31\u001b[0m                                        neuralCube, \n\u001b[1;32m     32\u001b[0m                                        sentenceDat[\u001b[39m'\u001b[39;49m\u001b[39msentencePrompt\u001b[39;49m\u001b[39m'\u001b[39;49m][:,\u001b[39m0\u001b[39;49m], \n\u001b[1;32m     33\u001b[0m                                        sentenceDat[\u001b[39m'\u001b[39;49m\u001b[39mnumTimeBinsPerSentence\u001b[39;49m\u001b[39m'\u001b[39;49m][:,\u001b[39m0\u001b[39;49m], \n\u001b[1;32m     34\u001b[0m                                        trainPartitionIdx, \n\u001b[1;32m     35\u001b[0m                                        charDef)\n\u001b[1;32m     37\u001b[0m \u001b[39m#add single letter examples\u001b[39;00m\n\u001b[1;32m     38\u001b[0m snippetDict \u001b[39m=\u001b[39m addSingleLetterSnippets(snippetDict, \n\u001b[1;32m     39\u001b[0m                                       singleLetterDat, \n\u001b[1;32m     40\u001b[0m                                       twCubes, \n\u001b[1;32m     41\u001b[0m                                       charDef)\n",
      "File \u001b[0;32m~/Desktop/universitaÌ€/Magisitrale/2023:2024/II semestre/Machine Learning/Progetto /BCI/BCI_HW/makeSyntheticSentences.py:265\u001b[0m, in \u001b[0;36mextractCharacterSnippets\u001b[0;34m(letterStarts, blankWindows, neuralCube, sentences, sentenceLens, trainPartitionIdx, charDef)\u001b[0m\n\u001b[1;32m    263\u001b[0m     bw \u001b[39m=\u001b[39m blankWindows[\u001b[39m0\u001b[39m, sentIdx]\n\u001b[1;32m    264\u001b[0m     \u001b[39mfor\u001b[39;00m blankIdx \u001b[39min\u001b[39;00m bw:\n\u001b[0;32m--> 265\u001b[0m         validIndices \u001b[39m=\u001b[39m blankIdx[blankIdx \u001b[39m<\u001b[39;49m neuralCube\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]]\n\u001b[1;32m    266\u001b[0m         snippetDict[\u001b[39m'\u001b[39m\u001b[39mblank\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(neuralCube[sentIdx, validIndices, :])\n\u001b[1;32m    268\u001b[0m \u001b[39mreturn\u001b[39;00m snippetDict\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "#First, we generate snippet libraries for each dataset by cutting out each letter from each sentence. These can then\n",
    "#be re-arranged into new sequences. \n",
    "for dataDir in dataDirs:\n",
    "    print('Processing ' + dataDir)\n",
    "    \n",
    "    for cvPart in cvParts:\n",
    "        print('--' + cvPart)\n",
    "        \n",
    "        #load datasets and train/test partition\n",
    "        sentenceDat = scipy.io.loadmat(rootDir+'Datasets/'+dataDir+'/sentences.mat')\n",
    "        singleLetterDat = scipy.io.loadmat(rootDir+'Datasets/'+dataDir+'/singleLetters.mat')\n",
    "        twCubes = scipy.io.loadmat(rootDir+'LSTMTrainingSteps/Step1_TimeWarping/'+dataDir+'_warpedCubes.mat')\n",
    "        \n",
    "        cvPartFile = scipy.io.loadmat('/Users/bishoyzakhary/handwritingBCIData/LSTMTrainingSteps/trainTestPartitions_HeldOutBlocks.mat')\n",
    "        trainPartitionIdx = cvPartFile[dataDir+'_train']\n",
    "        \n",
    "        #the last two sessions have hashmarks (#) to indicate that T5 should take a brief pause\n",
    "        #here we remove these from the sentence prompts, otherwise the code below will get confused (because # isn't a character)\n",
    "        for x in range(sentenceDat['sentencePrompt'].shape[0]):\n",
    "            sentenceDat['sentencePrompt'][x,0][0] = sentenceDat['sentencePrompt'][x,0][0].replace('#','')\n",
    "        \n",
    "        #normalize the neural activity cube\n",
    "        neuralCube = normalizeSentenceDataCube(sentenceDat, singleLetterDat)\n",
    "        \n",
    "        #load labels\n",
    "        labels = scipy.io.loadmat(rootDir + 'LSTMTrainingSteps/Step2_HMMLabels/'+cvPart+'/'+dataDir+'_timeSeriesLabels.mat')\n",
    "\n",
    "        #cut out character snippets from the data for augmentation\n",
    "        snippetDict = extractCharacterSnippets(labels['letterStarts'], \n",
    "                                               labels['blankWindows'], \n",
    "                                               neuralCube, \n",
    "                                               sentenceDat['sentencePrompt'][:,0], \n",
    "                                               sentenceDat['numTimeBinsPerSentence'][:,0], \n",
    "                                               trainPartitionIdx, \n",
    "                                               charDef)\n",
    "\n",
    "        #add single letter examples\n",
    "        snippetDict = addSingleLetterSnippets(snippetDict, \n",
    "                                              singleLetterDat, \n",
    "                                              twCubes, \n",
    "                                              charDef)\n",
    "\n",
    "        #save results\n",
    "        if not os.path.isdir(rootDir + 'LSTMTrainingSteps/Step3_SyntheticSentences/'+cvPart):\n",
    "            os.mkdir(rootDir + 'LSTMTrainingSteps/Step3_SyntheticSentences/'+cvPart)\n",
    "        scipy.io.savemat(rootDir + 'LSTMTrainingSteps/Step3_SyntheticSentences/'+cvPart+'/'+dataDir+'_snippets.mat', snippetDict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we use the above snippet libraries to make synthetic data for each dataset and train/test partition.\n",
    "\n",
    "#'nParallelProcesses' specifies how many parallel processes to use when generating synthetic data (to speed things up).\n",
    "#Decrease if it uses too much memory on your machine. (10 uses ~30 GB of RAM)\n",
    "nParallelProcesses = 10\n",
    "\n",
    "for dataDir in dataDirs:\n",
    "    print('Processing ' + dataDir)\n",
    "    \n",
    "    for cvPart in cvParts:\n",
    "        print('--' + cvPart)\n",
    "        \n",
    "        outputDir = rootDir+'LSTMTrainingSteps/Step3_SyntheticSentences/'+cvPart+'/'+dataDir+'_syntheticSentences'\n",
    "        bashDir = rootDir+'bashScratch'\n",
    "        repoDir = os.getcwd()\n",
    "\n",
    "        if not os.path.isdir(outputDir):\n",
    "            os.mkdir(outputDir)\n",
    "\n",
    "        if not os.path.isdir(bashDir):\n",
    "            os.mkdir(bashDir)\n",
    "\n",
    "        args = {}\n",
    "        args['nSentences'] = 256\n",
    "        args['nSteps'] = 2400\n",
    "        args['binSize'] = 2\n",
    "        args['wordListFile'] = repoDir+'/wordList/google-10000-english-usa.txt' #from https://github.com/first20hours/google-10000-english\n",
    "        args['rareWordFile'] = repoDir+'/wordList/rareWordIdx.mat'\n",
    "        args['snippetFile'] = rootDir+'LSTMTrainingSteps/Step3_SyntheticSentences/'+cvPart+'/'+dataDir+'_snippets.mat'\n",
    "        args['accountForPenState'] = 1\n",
    "        args['charDef'] = getHandwritingCharacterDefinitions()\n",
    "        args['seed'] = datetime.datetime.now().microsecond\n",
    "\n",
    "        argList = []\n",
    "        for x in range(20):\n",
    "            newArgs = args.copy()\n",
    "            newArgs['saveFile'] = outputDir+'/bat_'+str(x)+'.tfrecord'\n",
    "            newArgs['seed'] += x\n",
    "            argList.append(newArgs)\n",
    "\n",
    "        pool = multiprocessing.Pool(nParallelProcesses)     \n",
    "        results = pool.map(generateCharacterSequences, argList)\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot an example synthetic sentence\n",
    "trlIdx = 22\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(3,1,1)\n",
    "plt.imshow(np.transpose(results[0][0][trlIdx,:,:]),aspect='auto',clim=[-1,1])\n",
    "plt.title('Synthetic Neural Data')\n",
    "plt.ylabel('Electrode #')\n",
    "plt.xlabel('Time Bin (20 ms)')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.imshow(np.transpose(results[0][1][trlIdx,:,0:-1]),aspect='auto')\n",
    "plt.title('Character Probability Targets')\n",
    "plt.ylabel('Character #')\n",
    "plt.xlabel('Time Bin (20 ms)')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(results[0][1][trlIdx,:,-1])\n",
    "plt.plot(results[0][2][trlIdx,:])\n",
    "plt.xlim([0,results[0][1].shape[1]])\n",
    "plt.title('Character Start Target & Error Weights')\n",
    "plt.legend(['Character Start Signal', 'Error Weight'])\n",
    "plt.xlabel('Time Bin (20 ms)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
